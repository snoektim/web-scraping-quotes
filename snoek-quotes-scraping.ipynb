{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Your Favorite Quotes from BrainyQuote using Python\n",
    "\n",
    "\n",
    "\n",
    "![banner_image](https://i.imgur.com/9rLptlu.png)\n",
    "\n",
    "\n",
    "### Web Scraping\n",
    "\n",
    "Web scraping is the extraction of information from web pages, typically in an automated fashion. There are several approaches to accomplish this. In this project, I will demonstrate the use of Python to scrape information from a website that harbors thousands of quotes. The method I outline relies primarily on the Python libraries `requests` and `BeautifulSoup`. The output of the project is one master function (and several underlying helper functions) in which simply the topic of interest is entered as an argument, resulting in a CSV file as output that harbors all the quotes belonging to the topic, together with the respective authors and links that lead directly to each quote.\n",
    "\n",
    "\n",
    "### BrainyQuote\n",
    "The website *BrainyQuote* claims to be the world's largest quotation site, and indeed forms an extensive reservoir of quotes. As  put on its website:\n",
    "\n",
    "*Originally published in 2001, BrainyQuote is one of the oldest and most established quotation sites on the web. Our site was built from scratch into the behemoth it is today. In the beginning, we used library books to enter famous quotations by hand. Armed with eyedrops and comfy wrist-rests at our computers, we typed, and typed, and typed! Today, you can enjoy the fruits of our labors; we are a shining example of the little engine that could.*\n",
    "\n",
    "Despite the large amounts of data that can be harnessed to provide novel insights, **quotes** remain a powerful way of capturing the essence of a phenomenon in a concise and appealing way. For that reason, book authors often use one or several quotes to start a chapter. A quote, in essence, consists of two parts: the exact quote, and the author of that quote. Although in theory a good quote stands on its own, in practice it is the combination of *what* is said and *who* said it that makes a quote powerful. Therefore, in this project we will extract both the exact quote as well as the person to whom the quote can be attributed to (the author).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "On [BrainyQuote.com](https://www.brainyquote.com/), quotes are categorized by author, by topic, and there are also the options to view the quote of the day or to use the search bar, as shown below:\n",
    "\n",
    "![site_outline](https://i.imgur.com/2l0ujWp.png)\n",
    "\n",
    "\n",
    "BrainyQuote is a great resource for browsing through quotes. However, it can be valuable to collect quotes for documentation, inspiration, or for further analysis. On the page https://www.brainyquote.com/topics an overview of all the available topics on the site can be found. In this project we will use *web scraping* to extract a subset of quotes of interest from this site using the Python libraries [Requests](https://docs.python-requests.org/en/latest/) and [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "The goal of this project is: to use web scraping to download all the quotes that belong to a certain topic. As an example, we will focus on the topic 'motivational' in order to find out the steps to be taken. Afterwards, we will derive a set of functions that can subsequently be used to scrape any topic of interest.\n",
    "\n",
    "The outline of the steps is given below:\n",
    "\n",
    "1. Identify the webpages\n",
    "2. Download a webpage using Requests\n",
    "3. Use Beautiful Soup to parse the HTML source code\n",
    "4. Extract author, quote text and url for each quote on the page\n",
    "5. Collect the downloaded data into Python lists\n",
    "6. Extract and combine data from multiple pages\n",
    "7. Create CSV file with the extracted information\n",
    "\n",
    "\n",
    "Ultimately, the results will be exported to a CSV file in the following format:\n",
    "\n",
    "```\n",
    "author, quote, url\n",
    "St. Jerome, Good, better, best. Never let it rest. 'Til your good is better and your better is best., https://www.brainyquote.com/quotes/st_jerome_389605?src=t_motivational\n",
    "Charles R. Swindoll, Life is 10% what happens to you and 90% how you react to it., https://www.brainyquote.com/quotes/charles_r_swindoll_388332?src=t_motivational\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Run the Code\n",
    "In order to execute the code, please use the \"Run\" button at the top of this page and select \"Run on Binder\". You can edit the notebook and save a personal version to [Jovian](https://wwww.jovian.ai) by executing the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"snoek-quotes-scraping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Identify the webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the page https://www.brainyquote.com/topics all topics are listed. For this project we will select quotes from the topic 'motivational':\n",
    "\n",
    "![motivational](https://i.imgur.com/5vbdIDP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the url of the resulting page, we find that the url is structured in the following way:\n",
    "\n",
    "`https://www.brainyquote.com/topics/motivational-quotes`\n",
    "\n",
    "Therefore, we will save the topic in a variable, which can then be used to construct the main url of the target page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'motivational'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of main url of the to be scraped page\n",
    "main_url = 'https://www.brainyquote.com/topics/' + topic + '-quotes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the url\n",
    "main_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visiting the above url, we notice that the site contains of in total 5 subpages. \n",
    "\n",
    "\n",
    "![subpages](https://i.imgur.com/kSWnMGt.png)\n",
    "\n",
    "In addition to the main page, these are:\n",
    "\n",
    "https://www.brainyquote.com/topics/motivational-quotes_2\n",
    "\n",
    "https://www.brainyquote.com/topics/motivational-quotes_3\n",
    "\n",
    "https://www.brainyquote.com/topics/motivational-quotes_4\n",
    "\n",
    "https://www.brainyquote.com/topics/motivational-quotes_5\n",
    "\n",
    "\n",
    "We will save the number of subpages in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the number of subpages\n",
    "nr_of_subpages = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this knowledge to create a list of URLs to be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a list \n",
    "urls = [main_url]\n",
    "base_url = main_url\n",
    "\n",
    "for i in range(2, nr_of_subpages + 1):\n",
    "    url = base_url + '_' + str(i)\n",
    "    urls.append(url)\n",
    "\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this in a function. In the function we will ask the user to check and then enter the number of sub pages for the topic of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_pages(topic):\n",
    "    # the main_url (i.e. the first page with quotes on the topic of interest) is generated\n",
    "    main_url = 'https://www.brainyquote.com/topics/' + topic + '-quotes'\n",
    "    \n",
    "    # the user is asked for input\n",
    "    nr_of_subpages = input(\"Enter the number of subpages of {}\".format(main_url))\n",
    "    \n",
    "    # we initialize a list of urls starting off with the main_url. The main_url is also used as a base_url\n",
    "    urls = [main_url]\n",
    "    base_url = main_url\n",
    "    \n",
    "    # we iterate over the number of subpages to generate the urls which we will scrape\n",
    "    for i in range(2, int(nr_of_subpages) + 1):\n",
    "        url = base_url + '_' + str(i)\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the function works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_topic_pages('motivational')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the function returns the correct urls which we will scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download a web page using `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can access the information on the website, the website needs to be downloaded. We will use the [`requests`](https://docs.python-requests.org/en/master/) library to download the web page. \n",
    "\n",
    "The library can be installed using `pip`, which stands for \"Python Installer Package\", and subsequently imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library\n",
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library is now installed and imported.\n",
    "\n",
    "We will first focus on collecting the quotes from the first subpage and store in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_url = urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `requests.get`returns a response object containing the data from the web page and some other information. We will save this in a variable called 'response':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(topic_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether the response was succesful, we access the `.status_code` property of the response object. A succesful response will yield an [HTTP status code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) between 200 and 299."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request was succesful.\n",
    "\n",
    "Access the contents of the web page using the `.text` property of the `response` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check out the page length (i.e. number of characters on the page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the page contains over 60,000 characters, we will limit the view to the first 500 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows us the [HTML source code](https://nl.wikipedia.org/wiki/HyperText_Markup_Language) of the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the page content to a file, which then allows us to view the page locally within Jupyter using \"File > Open\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('webpage.html', 'w') as f:\n",
    "    f.write(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When opening the downloaded page, one would typically see the original page with none of the links working. In this case we see that the downloaded page does not load as the original:\n",
    "\n",
    "![html page](https://i.imgur.com/In5gXD6.png)\n",
    "\n",
    "This is likely caused by the presence of advertisement on the page that are dynamically loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we used the `requests` library to download a web page as HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use `BeautifulSoup` to parse the HTML source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have downloaded the web page, the next step is to locate the information we require within the HTML code of the site.\n",
    "\n",
    "We will use the [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library to extract information from the HTML source code. First we will install the library and then import the `BeautifulSoup` class from the `bs4` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a `BeautifulSoup` object that will contain the parsed content of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = BeautifulSoup(page_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of Beautiful Soup is that the resulting `doc` object possesses several properties and method to extract data from the HTML document. For example, we can extract the title of the page using `doc.title`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tag = doc.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the title by extracting the text of the tag using `.text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tag.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, when a certain tag occurs more than once in the document (for example an `img` tag) we can use `doc.img` to find the first occurence of this tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image = doc.img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very powerful usage of Beautiful Soup is to find all the tags of the same type within `doc` using the `find_all` method. This will be demonstrated in the next section of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_parse(topic_url):\n",
    "    '''Download a web page and return a Beautiful Soup doc'''\n",
    "    \n",
    "    # download the page\n",
    "    response = requests.get(topic_url)\n",
    "    \n",
    "    # check if download was succesful; raise an exception if not\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"loading problem with {}\".format(topic_url))\n",
    "    \n",
    "    # get the page HTML\n",
    "    page_content = response.text\n",
    "    \n",
    "    # create a bs4 doc\n",
    "    doc = BeautifulSoup(page_content, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether the function returns the same output as we found above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = download_and_parse('https://www.brainyquote.com/topics/motivational-quotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.title == doc2.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the function `download_and_parse` to download any web page and parse it using Beautiful Soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract author, quote text and URL for each quote on the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the information we require can be found by inspecting the HTML code of the page. The basic structure of an HTML document consists of tags, such as `html`, `head`, `body` and `title` tags. In essence, tags mark the beginning and end of an *element*. For example, a `title` element consists of the opening tag `<title>`, followed by the content, and closes with `</title` as we can also see by inspecting the first of few lines of the downloaded page (webpage.html) with Notepad Plus: ![html_in_Notepad](https://i.imgur.com/G8bFQwi.png)\n",
    "\n",
    "Here we see the following:\n",
    "- `<!DOCTYPE html>`: this is the document type declaration, and tells the browser the type of HTML that is being used (in  this case: HTML5)\n",
    "- `<html lang=\"en\">`: this `html` opening tag indicates that the page is written in HTML. The corresponding closing tag (`</html>`) can be found at the very end of the page:\n",
    "\n",
    "![end_of_page](https://i.imgur.com/WDrFf8u.png)\n",
    "\n",
    "- `<head>`: the `head` tag indicates the beginning of the section that contains information *about* the page that will not appear in the browser\n",
    "- `<title>Motivational Quotes - BrainyQuote</title>`: within the `head` section the `title` element is found, which specifies the title of the page (e.g. the title that is displayed in title bar of the browser window).\n",
    "\n",
    "Although most opening tags are followed by closing tags, there are also tags that do not require a closing tag, such as `img` and `br` tags. As a rule of thumb, tags with content between them should be closed. Taken together, the basic structure of most elements is the following: opening tag - content - closing tag. We can use this knowledge to find the information we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags can have *attributes*, the function of which is to modify the behavior or the display of the element. The attributes are located inside the opening tag and the values are specified within quotation marks. Examples of attributes are: `href` (used within `a` tags), `id` (can be added to almost all tags), `src` (to be used with `img` tags), `style` and `class`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author Names\n",
    "\n",
    "Let us first focus on the Author Names. We will navigate to the first page containing quotes in Chrome (https://www.brainyquote.com/topics/motivational-quotes), locate a quote, hover over the author name, right-mouse click and then select \"inspect\" option:\n",
    "\n",
    "![inspect_element](https://i.imgur.com/AQBhDQn.png)\n",
    "\n",
    "Now we can inspect the html code of the part of the page that displays the author name in detail. This teaches us that the title is within an `a` tag:\n",
    "\n",
    "![html_author](https://i.imgur.com/cglxzRf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`a` tags are so-called 'anchor' tags, and are used to define links. As we see here, the tag contains an `href` attribute that specifies the destination of the link (i.e.: \"/authors/st-jerome-quotes\"). In this case, the author name on the page is indeed a clickable link, that re-directs us to the page where all the quotes of this particular author (St. Jerome) are grouped:\n",
    "![St_Jerome_page](https://i.imgur.com/rkKFSOD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first attempt to obtain the tags containing the author names, let us collect all `a` tags from the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = doc.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many tags we have collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected 195 a tags. This is more than there are quotes on the page.  Let us inspect the html code further by inspecting the underlying code (using the \"element\" button) for three authors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<a href=\"/authors/st-jerome-quotes\" class=\"bq-aut qa_389605 oncl_a\" title=\"view author\">St. Jerome</a>`\n",
    "\n",
    "`<a href=\"/authors/charles-r-swindoll-quotes\" class=\"bq-aut qa_388332 oncl_a\" title=\"view author\">Charles R. Swindoll</a>`\n",
    "\n",
    "`<a href=\"/authors/walt-disney-quotes\" class=\"bq-aut qa_130027 oncl_a\" title=\"view author\">Walt Disney</a>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that author names are embedded in `a` tags having the class `bq-aut qa_`. We will try to select a tags belonging to this class and then check out the number of collected tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_tags = doc.find_all('a', class_ = \"bq-aut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(author_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browsing through the first subpage (out of 5), there seem to be indeed 60 quotes per page. Let us check out the first five author_tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_tags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the author name out of the tag using \".text\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_tags[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function that collects all the author names from a page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_info(doc):\n",
    "    author_tags = doc.find_all('a', class_ = \"bq-aut\")\n",
    "    authors = [tag.text for tag in author_tags]\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify the function by displaying the first five results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = get_author_info(doc)\n",
    "authors[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the number of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function appears to return 60 author names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quotes\n",
    "\n",
    "Analogous to the method followed for the author names, we use the \"Inspect\" function in the browser to investigate the html code underlying quotes::\n",
    "\n",
    "![inspect_quote](https://i.imgur.com/M8hGO7B.png)\n",
    "\n",
    "\n",
    "It appears that the quote is embedded in a so-called `div` tag, which is itself embedded within an `a` tag:\n",
    "\n",
    "`<div style=\"display: flex;justify-content: space-between\">`\n",
    "\n",
    "Note that `div` tags, unlike most other tags, do not apply a particular meaning. `div` (division) elements are basicually used to group larger pieces of code together, and in practice will result in a line-break before and after it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to collect these `div` tags directly, by specifing their `style` attribute as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_tags = doc.find_all('div', {'style': 'display: flex;justify-content: space-between'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quote_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here we get about 60 hits. Let's verify these tags contain indeed quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_tags[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rid of the newline characters (\\n) we have to apply the strip method to the tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_tags[1].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to collect all the quotes in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quote_info(doc):\n",
    "    quote_tags = doc.find_all('div', {'style': 'display: flex;justify-content: space-between'})\n",
    "    quotes = [tag.text.strip() for tag in quote_tags]\n",
    "    return quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we check the functionality of the function by displaying the first three quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quotes = get_quote_info(doc)\n",
    "quotes[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition we check the number of quotes collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_quote_info(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function indeed appears to return 60 quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs\n",
    "\n",
    "In order to collect the URL leading to the quote, we can take advantage of the inspection of the HTML code we carried for the retrieval of the Quotes (see above). We already noticed that the link to a page displaying the quote is embedded with an `a` tag:\n",
    "\n",
    "`<a href=\"/quotes/st_jerome_389605?src=t_motivational\" class=\"b-qt qt_389605 oncl_q\" title=\"view quote\">`\n",
    "\n",
    "\n",
    "The `class` attribute is used for layout and styling. Note that since `class` is a reserved keyword in Python, we have to use `class_` here in order to extract the `a` tags from the class 'b-qt':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_tags = doc.find_all('a', class_ = \"b-qt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(link_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number seems to be correct. Let us inspect the first tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the part of tag leading to the page with the quote by accessing the `href` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_tags[0]['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can re-create the full URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the base_url\n",
    "base_url = 'https://www.brainyquote.com'\n",
    "\n",
    "# then generate the quote of the first url\n",
    "topic0_url = base_url + link_tags[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking the link leads us to a page displaying the quote, a nice background as well as additional information:\n",
    "\n",
    "![quote_example](https://i.imgur.com/b5Wszxp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put the above in a function that scrapes the URL of each quote and collects them in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_info(doc):\n",
    "    link_tags = doc.find_all('a', class_ = \"b-qt\")\n",
    "    base_url = 'https://www.brainyquote.com'\n",
    "    quote_urls = [base_url + tag['href'] for tag in link_tags]\n",
    "    return quote_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_urls = get_link_info(doc)\n",
    "quote_urls[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quote_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns 60 URLs, each leading to a quote page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collect the downloaded data into Python lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate a function that for a single page generates a doc file, and then collects the required data (i.e. authors, quotes, and URLs). This function will call the functions we have defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes_per_page(url):\n",
    "    doc = download_and_parse(url)\n",
    "    authors = get_author_info(doc)\n",
    "    quotes = get_quote_info(doc)\n",
    "    quote_urls = get_link_info(doc)\n",
    "    return authors, quotes, quote_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify the function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_quotes_per_page = get_quotes_per_page('https://www.brainyquote.com/topics/motivational-quotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_quotes_per_page[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_quotes_per_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected 3 lists, let us verify the length and first three items of each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first list contains {} items, the first 3 are:'.format(len(test_quotes_per_page[0])), test_quotes_per_page[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The second list contains {} items, the first 3 are:'.format(len(test_quotes_per_page[1])), test_quotes_per_page[1][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The third list contains {} items, the first 3 are:'.format(len(test_quotes_per_page[2])), test_quotes_per_page[2][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have verified that the function `get_quotes_per_page ` for a single page generates a doc file, and then collects the required data (i.e. authors, quotes, and URLs) as three separate lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract and combine data from multiple pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect all the quotes belonging to a certain topic, we have to ensure all subpages are scraped subsequently. To this end we define a function that first calls the function `get_topic_pages` to generate the set of to-be-scraped urls, and then for each url calls the function `get_quotes_per_page`, which we defined in the previous section. Note that this function also calls the function `get_output_file`, which we will define in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping(topic):\n",
    "    # empty lists are initialized in which all authors, quotes, and quote urls will be collected\n",
    "    all_authors, all_quotes, all_quote_urls  = [], [], []\n",
    "    \n",
    "    # call 'get_topic_pages' to obtain a set of to-be-scraped URLs\n",
    "    urls = get_topic_pages(topic)\n",
    "    \n",
    "    # loop over the URLs\n",
    "    for url in urls:\n",
    "        # for each URL collect the lists of authors, quotes and URLs\n",
    "        authors, quotes, quote_urls = get_quotes_per_page(url)\n",
    "        \n",
    "        # add the collected lists of data to the master lists (i.e. all_auhors, all_quotes, all_quote_urls)\n",
    "        all_authors += authors\n",
    "        all_quotes += quotes\n",
    "        all_quote_urls += quote_urls\n",
    "        \n",
    "    # write all the collected data to a |csv file by calling the function 'get_outout_file'\n",
    "    get_output_file(all_authors, all_quotes, all_quote_urls, topic) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create CSV file with the extracted information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have collected all the relevant data, we will use the Pandas library in order to create a dataframe from our collected data. First we install and import pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library has now been installed and imported. Now we create a dictionary, with 'author', 'quotes' and 'urls' as keys, and the collected data (in lists) as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_dict = {\n",
    "    'author': authors,\n",
    "    'quote': quotes,\n",
    "    'url': quote_urls\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary we have created will now be converted into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_df = pd.DataFrame(quotes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the first five rows of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we ensure the topic of interest is included in the filename, and then we write the dataframe to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = topic + '-quotes.csv'\n",
    "\n",
    "# \"index=None\" in order to not include the row numbers in the file:\n",
    "quotes_df.to_csv(filename, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the first five entries of the CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head motivational-quotes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now reached the final goal of the project; i.e. we exported the results will to a CSV file in the desired format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file(authors, quotes, quote_urls):\n",
    "    filename = topic + '-quotes.csv'\n",
    "    \n",
    "    quotes_dict = {\n",
    "        'author': authors,\n",
    "        'quote': quotes,\n",
    "        'url': quote_urls}\n",
    "    quotes_df = pd.DataFrame(quotes_dict)\n",
    "    quotes_df.to_csv(filename, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here's what we have covered:\n",
    "\n",
    "1. Identify the webpages\n",
    "2. Download a webpage using `requests`\n",
    "3. Use `Beautiful Soup` to parse the HTML source code\n",
    "4. Extract author, quote text and url for each quote on the page\n",
    "5. Collect the downloaded data into Python lists\n",
    "6. Extract and combine data from multiple pages\n",
    "7. Create CSV file with the extracted information\n",
    "\n",
    "The CSV file we created has this format\n",
    "\n",
    "```\n",
    "author, quote, url\n",
    "St. Jerome, Good, better, best. Never let it rest. 'Til your good is better and your better is best., https://www.brainyquote.com/quotes/st_jerome_389605?src=t_motivational\n",
    "Charles R. Swindoll, Life is 10% what happens to you and 90% how you react to it., https://www.brainyquote.com/quotes/charles_r_swindoll_388332?src=t_motivational\n",
    "```\n",
    "\n",
    "Here is the complete code for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "  \n",
    "\n",
    "# This function collects all info from all urls belonging to a certain topic\n",
    "def scraping(topic):\n",
    "    all_authors, all_quotes, all_quote_urls  = [], [], []\n",
    "    \n",
    "    urls = get_topic_pages(topic)\n",
    "    \n",
    "    for url in urls:\n",
    "        authors, quotes, quote_urls = get_quotes_per_page(url)\n",
    "        all_authors += authors\n",
    "        all_quotes += quotes\n",
    "        all_quote_urls += quote_urls\n",
    "        \n",
    "    get_output_file(all_authors, all_quotes, all_quote_urls, topic)    \n",
    "\n",
    "\n",
    "\n",
    "# This function generates the set of to-be-scraped urls\n",
    "def get_topic_pages(topic):\n",
    "    main_url = 'https://www.brainyquote.com/topics/' + topic + '-quotes'\n",
    "    nr_of_subpages = input(\"Enter the number of subpages of {}\".format(main_url))\n",
    "    urls = [main_url]\n",
    "    base_url = main_url\n",
    "    for i in range(2, int(nr_of_subpages) + 1):\n",
    "        url = base_url + '_' + str(i)\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "\n",
    "# This function should get the data from one page\n",
    "def get_quotes_per_page(url):\n",
    "    doc = download_and_parse(url)\n",
    "    authors = get_author_info(doc)\n",
    "    quotes = get_quote_info(doc)\n",
    "    quote_urls = get_link_info(doc)\n",
    "    return authors, quotes, quote_urls\n",
    "\n",
    "\n",
    "# This function parses the sites and generates doc\n",
    "def download_and_parse(topic_url):\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"loading problem with {}\".format(topic_url))\n",
    "    page_content = response.text\n",
    "    doc = BeautifulSoup(page_content, 'html.parser')\n",
    "    return doc \n",
    "    \n",
    "# Gets all the author info from the doc as a list    \n",
    "def get_author_info(doc):\n",
    "    author_tags = doc.find_all('a', class_ = \"bq-aut\")\n",
    "    authors = [tag.text for tag in author_tags]\n",
    "    return authors\n",
    "  \n",
    "# Gets all the quotes from the doc as a list    \n",
    "def get_quote_info(doc):\n",
    "    quote_tags = doc.find_all('div', {'style': 'display: flex;justify-content: space-between'})\n",
    "    quotes = [tag.text.strip() for tag in quote_tags]\n",
    "    return quotes\n",
    "\n",
    "# Gets all the quote urls from the doc as a list \n",
    "def get_link_info(doc):\n",
    "    link_tags = doc.find_all('a', class_ = \"b-qt\")\n",
    "    base_url = 'https://www.brainyquote.com'\n",
    "    quote_urls = [base_url + tag['href'] for tag in link_tags]\n",
    "    return quote_urls\n",
    "\n",
    "# Creates a dictionary, converts it to a df, and then writes output to csv file\n",
    "def get_output_file(authors, quotes, quote_urls, topic):\n",
    "    quotes_dict = {\n",
    "        'author': authors,\n",
    "        'quote': quotes,\n",
    "        'url': quote_urls}\n",
    "    quotes_df = pd.DataFrame(quotes_dict)\n",
    "    quotes_df.to_csv(topic + '-quotes.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify this for the topic 'knowledge'. We only have to call the master function `scraping` and provide 'knowledge' as the argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping('knowledge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice the page `https://www.brainyquote.com/topics/knowledge-quotes`has 17 subpages, which we enter as input:\n",
    "![knowledge](https://i.imgur.com/zRlhLRt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that a CSV file has been generated named 'knowledge-quotes.csv' and inspect the first five lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head knowledge-quotes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we open the CSV file directly to verify all 17 pages have been scraped. Since there are 60 quotes per page, the file should contain 960 - 1020 quotes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knowledge_csv](https://i.imgur.com/zdwueVt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the CSV file contains 1001 lines, indicating 17 pages have been succesfully scraped for the topic 'knowledge'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will re-call the function for our original topic 'motivational':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping('motivational')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also now we open the CSV file directly to verify all 5 pages have been scraped. Since there are 60 quotes per page, the file should contain 240 - 300 quotes:\n",
    "\n",
    "![motivational](https://i.imgur.com/xVzebKZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the CSV file contains 283 lines, indicating that five pages have been succesfully scraped for the topic 'motivational'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save this notebook together with the generated CSV files for 'motivational' and 'knowledge'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Execute this to save new versions of the notebook including the csv files)\n",
    "jovian.commit(files = ['motivational-quotes.csv', 'knowledge-quotes.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we conclude this notebook with a [quote](https://www.brainyquote.com/quotes/nelson_mandela_378967?img=4&src=t_motivational):\n",
    "\n",
    "![Mandela](https://i.imgur.com/wWA2yuO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "* We can now fetch individual topic pages and get all the quotes. Further refinement of this notebook could include an option to scrape quotes from a particular author, the quotes of the day, or even the quotes resulting from specific search queries.\n",
    "* The current notebook requires user input (i.e. entering the number of subpages). The notebook could be further improved by automating this step as well.\n",
    "* With the collected data, further analysis can be carried out. For example the most frequently used words per topic could be determined, and then it could be analyzed how this differs among topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Jovian Web Scraping Tutorial](https://jovian.ai/learn/zero-to-data-analyst-bootcamp/lesson/web-scraping-and-rest-apis)\n",
    "- [Requests documentation](https://docs.python-requests.org/en/latest/)\n",
    "- [Beautiful Soup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html)\n",
    "- [HTML Tutorials by HTML Dog](https://htmldog.com/guides/html/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
